{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "debb41c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T13:25:14.219941Z",
     "start_time": "2023-05-03T13:25:14.092562Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 312\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m## 네트워크 학습하기위해 data load 부분\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([Normalization(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\u001b[43mRandomFlip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,ToTensor()])\n\u001b[1;32m    313\u001b[0m test_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([Normalization(mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),ToTensor()])\n\u001b[1;32m    315\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m Dataset(data_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), transform\u001b[38;5;241m=\u001b[39mtrain_transform)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "\n",
    "# 데이터 불러오기\n",
    "dir_data = './datasets'\n",
    "\n",
    "name_label = 'train-labels.tif'\n",
    "name_input = 'train-volume.tif'\n",
    "\n",
    "img_label = Image.open(os.path.join(dir_data, name_label))\n",
    "img_input = Image.open(os.path.join(dir_data, name_input))\n",
    "\n",
    "ny, nx = img_label.size                  # 512 * 512\n",
    "nframe = img_label.n_frames              # 30 frame\n",
    "\n",
    "# dataset을 Train과 Test로 나누기\n",
    "nframe_train = 24\n",
    "nframe_val = 3\n",
    "nframe_test = 3\n",
    "\n",
    "# data가 저장될 Directory 설정하기\n",
    "dir_save_train = os.path.join(dir_data, 'train')\n",
    "dir_save_val = os.path.join(dir_data, 'val')\n",
    "dir_save_test = os.path.join(dir_data, 'test')\n",
    "\n",
    "# Dir 생성\n",
    "if not os.path.exists(dir_save_train) :\n",
    "    os.makedirs(dir_save_train)\n",
    "if not os.path.exists(dir_save_val) :\n",
    "    os.makedirs(dir_save_val)\n",
    "if not os.path.exists(dir_save_test) :\n",
    "    os.makedirs(dir_save_test)\n",
    "    \n",
    "# data dir에 dataset을 Random 하게 저장하기\n",
    "id_frame = np.arange(nframe)\n",
    "np.random.shuffle(id_frame)\n",
    "\n",
    "#train set 저장하기\n",
    "offset_nframe = 0\n",
    "\n",
    "for i in range(nframe_train) :\n",
    "    img_label.seek(id_frame[i + offset_nframe])\n",
    "    img_input.seek(id_frame[i + offset_nframe])\n",
    "    \n",
    "    label_ = np.asarray(img_label)\n",
    "    input_ = np.asarray(img_input)\n",
    "    \n",
    "    np.save(os.path.join(dir_save_train, 'label_%03d.npy' % i), label_)\n",
    "    np.save(os.path.join(dir_save_train, 'label_%03d.npy' % i), input_)\n",
    "    \n",
    "# val set 저장하기\n",
    "offset_nframe += nframe_train\n",
    "\n",
    "for i in range(nframe_val) :\n",
    "    img_label.seek(id_frame[i + offset_nframe])\n",
    "    img_input.seek(id_frame[i + offset_nframe])\n",
    "    \n",
    "    label_ = np.asarray(img_label)\n",
    "    input_ = np.asarray(img_input)\n",
    "    \n",
    "    np.save(os.path.join(dir_save_val, 'label_%03d.npy' % i), label_)\n",
    "    np.save(os.path.join(dir_save_val, 'label_%03d.npy' % i), input_)\n",
    "    \n",
    "# test set 저장하기\n",
    "offset_nframe += nframe_val\n",
    "\n",
    "for i in range(nframe_test) :\n",
    "    img_label.seek(id_frame[i + offset_nframe])\n",
    "    img_input.seek(id_frame[i + offset_nframe])\n",
    "    \n",
    "    label_ = np.asarray(img_label)\n",
    "    input_ = np.asarray(img_input)\n",
    "    \n",
    "    np.save(os.path.join(dir_save_test, 'label_%03d.npy' % i), label_)\n",
    "    np.save(os.path.join(dir_save_test, 'label_%03d.npy' % i), input_)\n",
    "\n",
    "# hyper parameter 설정\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epoch = 100\n",
    "\n",
    "data_dir = './datasets'\n",
    "ckpt_dir = './checkpoint'   #train된 Network가 저장될 dir\n",
    "log_dir = './log'           #tensorboard의 로그가 기록될 dir\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Layer 생성하기\n",
    "# UNet 네트워크에 nn.Module을 상속하기\n",
    "class UNet(nn.Module) :      \n",
    "    def __init__(self) :\n",
    "        super(UNet, self).__init__()  # 상속 초기화\n",
    "        \n",
    "        # Convolution Batch-nomarlization ReLU 2D\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True) :\n",
    "            # Convolution Layer 정의하기\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                bias=bias)]\n",
    "            # Batch-nomarlization 정희하기\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            # ReLU 정의하기\n",
    "            layers += [nn.ReLU()]\n",
    "            \n",
    "            cbr = nn.Sequential(*layers)\n",
    "            \n",
    "            return cbr\n",
    "        \n",
    "        # Contracting path (Encoder 부분)\n",
    "        # kernel_size=3, stride=1, padding=1, bias=True 생략 가능\n",
    "        self.enc1_1 = CBR2d(in_channels=1, out_channels=64)\n",
    "        self.enc1_2 = CBR2d(in_channels=64, out_channels=64)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc2_1 = CBR2d(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc3_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc4_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "        \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.enc5_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        \n",
    "        # Expansive path (Decoder 부분)\n",
    "        self.dec5_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "        \n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec4_2 = CBR2d(in_channels=512 * 2, out_channels=512) # *2 하는 이유는 UNet의 해당 Decoder부분 그림 잘 보기\n",
    "        self.dec4_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "        \n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec3_2 = CBR2d(in_channels=256 * 2, out_channels=256)\n",
    "        self.dec3_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "        \n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec2_2 = CBR2d(in_channels=128 * 2, out_channels=128)\n",
    "        self.dec2_1 = CBR2d(in_channels=128, out_channels=64)\n",
    "        \n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        self.dec1_2 = CBR2d(in_channels=64 * 2, out_channels=64)\n",
    "        self.dec1_1 = CBR2d(in_channels=64, out_channels=64)\n",
    "        \n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=2, stride=2, padding=0, bias=True)\n",
    "        \n",
    "        \n",
    "    # UNet 레이어 연결하기\n",
    "    def forward(self, x) :\n",
    "        # Encoder 부분 연결하기\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "        \n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2) \n",
    "        \n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "        \n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        \n",
    "        # Decoder 부분 연결하기\n",
    "        \n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "        \n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1) # dim=[0:batch, 1:channel, 2:height, 3:width]\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "        \n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "        \n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "        \n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "        \n",
    "        x = self.fc(dec1_1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "## 데이터 로더를 구현하기\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        lst_data = os.listdir(self.data_dir)\n",
    "\n",
    "        lst_label = [f for f in lst_data if f.startswith('label')]\n",
    "        lst_input = [f for f in lst_data if f.startswith('input')]\n",
    "\n",
    "        lst_label.sort()\n",
    "        lst_input.sort()\n",
    "\n",
    "        self.lst_label = lst_label\n",
    "        self.lst_input = lst_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = np.load(os.path.join(self.data_dir, self.lst_label[index]))\n",
    "        input = np.load(os.path.join(self.data_dir, self.lst_input[index]))\n",
    "\n",
    "        label = label/255.0\n",
    "        input = input/255.0\n",
    "\n",
    "        if label.ndim == 2:\n",
    "            label = label[:, :, np.newaxis]\n",
    "        if input.ndim == 2:\n",
    "            input = input[:, :, np.newaxis]\n",
    "\n",
    "        data = {'input': input, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# ToTensor() : numpy -> tensor로 변환\n",
    "\n",
    "class ToTensor(object) :\n",
    "    def __call__(self, data) :\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        # Image의 Numpy 차원 = (Y,X,ch)\n",
    "        # Image의 tensor 차원 = (ch,Y,X)\n",
    "        label = label.transpose((2,0,1)).astype(np.float32)\n",
    "        input = input.transpose((2,0,1)).astype(np.float32)\n",
    "        \n",
    "        data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
    "        \n",
    "        return data\n",
    "\n",
    "class Normalization(object) :\n",
    "    def __init__(self, mean=0.5, std=0.5) :\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, data) :\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        input = (input - self.mean) / self.std\n",
    "        data = {'label' :label, 'input':input}\n",
    "        \n",
    "        return data\n",
    "    \n",
    "class RandomFlip(object) :\n",
    "    def __init__ (self, data) :\n",
    "        label, input = data['label'], data['input']\n",
    "        \n",
    "        if np.random.rand() > 0.5: # 50% 확률\n",
    "            label = np.fliplr(label)\n",
    "            input = np.fliplr(input)\n",
    "        \n",
    "        if np.random.rand() > 0.5 :\n",
    "            label = np.flipud(label)\n",
    "            input = np.flipud(input)\n",
    "        \n",
    "        data = {'label' :label, 'input':input}\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 네트워크 학습하기위해 data load 부분\n",
    "train_transform = transforms.Compose([Normalization(mean=0.5, std=0.5),RandomFlip(),ToTensor()])\n",
    "test_transform = transforms.Compose([Normalization(mean=0.5, std=0.5),ToTensor()])\n",
    "\n",
    "dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=train_transform)\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "dataset_test = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=test_transform)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "## 네트워크 생성하기\n",
    "net = UNet().to(device)\n",
    "\n",
    "## 손실함수 정의하기\n",
    "fn_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "## Optimizer 설정하기\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "## 그밖에 부수적인 variables 설정하기\n",
    "num_data_test = len(dataset_test)\n",
    "num_data_train = len(dataset_train)\n",
    "num_batch_train = np.ceil(num_data_test / batch_size)\n",
    "\n",
    "    # 그밖에 부수적인 variables 설정하기\n",
    "\n",
    "\n",
    "## 그밖에 부수적인 functions 설정하기\n",
    "# tensor variable에서 Numpy로 변환시키는 함수\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "# normalization 되어있는 data를 반대로 de-normalization 하는 함수\n",
    "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
    "# 네트워크 아웃풋 이미지를 bin 클래스로 분류해주는 함수\n",
    "fn_class = lambda x: 1.0 * (x > 0.5)\n",
    "\n",
    "\n",
    "# 텐서보드를 사용하기 위한 summaryWriter 설정\n",
    "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
    "writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
    "       \n",
    "    \n",
    "## 네트워크 저장하기\n",
    "def save(ckpt_dir, net, optim, epoch):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    torch.save({'net': net.state_dict(), 'optim': optim.state_dict()},\n",
    "               \"./%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n",
    "\n",
    "## 네트워크 불러오기\n",
    "st_epoch = 0\n",
    "\n",
    "def load(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        epoch = 0\n",
    "        return net, optim, epoch\n",
    "\n",
    "    ckpt_lst = os.listdir(ckpt_dir)\n",
    "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    dict_model = torch.load('./%s/%s' % (ckpt_dir, ckpt_lst[-1]))\n",
    "\n",
    "    net.load_state_dict(dict_model['net'])\n",
    "    optim.load_state_dict(dict_model['optim'])\n",
    "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
    "\n",
    "    return net, optim, epoch\n",
    "\n",
    "# train을 진행시키는 반복문\n",
    "st_epoch = 0\n",
    "# 학습 이전에, 저장되어있는 network가 있다면 불러와서 연속적으로 학습할 수 있도록 load 해줌\n",
    "net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim )\n",
    "\n",
    "for epoch in range(st_epoch + 1, num_epoch + 1) :\n",
    "    net.train()\n",
    "    loss_arr = []\n",
    "    \n",
    "    for batch, data in enumerate(loader_train, 1):\n",
    "        # forward pass\n",
    "        # Netword의 input을 받아 Output을 출력\n",
    "        label = data['label'].to(device)\n",
    "        input = data['input'].to(device)\n",
    "        \n",
    "        output = net(input)\n",
    "        # backward\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss = fn_loss(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        # 손실함수 계산\n",
    "        loss_arr += [loss.item()]\n",
    "        print(\"TRAIN: EPOCH %04d / %04d BATCH %04d / %04d| LOSS %.4f\" %\n",
    "              (epoch,num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n",
    "        \n",
    "        # tensorboard에 input & out & label 저장 구문\n",
    "        label = fn_tonumpy(label)\n",
    "        input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
    "        output = fn_tonumpy(fn_class(output))\n",
    "        \n",
    "        writer_train.add_imgae('label', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "        writer_train.add_imgae('input', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "        writer_train.add_imgae('output', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "        \n",
    "        # Save the loss in tensorboard\n",
    "    writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
    "\n",
    "    \n",
    "    # Network validation\n",
    "    # backward가 없어, 사전에 막기 위해 torch.no_grad() 실시\n",
    "    with torch.no_grad() :\n",
    "        net.eval()               # Network validation 명시를 위해\n",
    "        loss_arr = []\n",
    "        \n",
    "        for batch, data in enumerate(loader_test, 1):\n",
    "            # forward pass\n",
    "            label = data['label'].to(device)\n",
    "            input = data['input'].to(device)    \n",
    "                \n",
    "            output = net(input)\n",
    "                \n",
    "            # 손실함수 계산\n",
    "            loss = fn_loss(output, label)\n",
    "                \n",
    "            loss_arr += [loss.item()]\n",
    "            print(\"VALID: EPOCH %04d / %04d BATCH %04d / %04d| LOSS %.4f\" %\n",
    "                  (epoch,num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n",
    "        \n",
    "            # tensorboard에 input & out & label 저장 구문\n",
    "            label = fn_tonumpy(label)\n",
    "            input = fn_tonumpy(fn_denorm(input))\n",
    "            output = fn_tonumpy(fn_class(output))\n",
    "\n",
    "            writer_val.add_imgae('label', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "            writer_val.add_imgae('input', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "            writer_val.add_imgae('output', label, num_batch_train * (epoch -1) + batch, dataformats = 'NHWC')\n",
    "          \n",
    "        # Save the loss in tensorboard\n",
    "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)   \n",
    "        \n",
    "        # epoch이 다섯번씩 진행될 때 마다 네트워크를 저장하는 부분\n",
    "        if epoch % 5 == 0:\n",
    "            save(ckpt_dir=ckpt_dir, net =net, optim = optim, epoch= epoch)\n",
    "\n",
    "writer_train.close()           \n",
    "writer_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506ac88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
